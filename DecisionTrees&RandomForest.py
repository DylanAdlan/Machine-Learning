'''
iexplore

1.
Decision theory always guarantees optimal solution. (False)

2.
output of a Random Forest classifier for a binary classification problem
- class labels

3.
not a part of decision tree problem specification
- Expected value of perfect information

4.
NOT a hyperparameter of Random Forest - Learning rate
Hyperparameter of Random Forest
- Number of estimators
- Max features
- Max depth

5.
Decision theory provides a rational and logical way of making decisions.(True)

6.
Random Forest handle missing values in the dataset by
- imputing missing values with the mean or median

7.
statements that's true about Random Forest
- It is less interpretable compared to decision trees

8. 
If a decision tree problem has 3 decision alternatives and 4 states of nature, the number of payoffs in that problem will be:
- 12

9.
purpose of the "out-of-bag" samples in Random Forest
- used to estimate the generalization error

10.
In pruning (evaluating) a decision tree, you write the expected payoff at a random outcome node 
inside the circle or square representing that node. (True)

ianalyse

1.
Bagging in the context of Random Forest
- Randomly selecting a subset of data for each tree

2.
metric that is used in splitting the nodes in decision tree algorithm
- information gain (entropy)

3.
In a Random Forest model, each tree is built using
- Decision Tree

4.
In a decision trees, a variable is splitting the parent node into two segments. First child node has entropy "0".
- It is good for the tree model

5.
Decision rules come from
- Final Leaf nodes

6.
Random Forest handle overfitting
- By randomly selecting features for each tree

7.
In Random Forest, purpose of bootstrapping
-Resampling the training dataset with replacement

8.
While calculating information gain in trees.
=> Information gain = entropy before split - weighted entropy after splitting

9.
Entropy in decision trees is
- Impurity or diversity measure

10.
What type of machine learning algorithm is Random Forest?
- Supervised Learning









'''